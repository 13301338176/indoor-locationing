model,fit_time (min),epochs,batch_size,hidden_layers,neurons_per_hidden_layer,L2_reg_lambda,dropout,train_accuracy,val_accuracy,train_accuracy - val_accuracy
ann,0.377539726,1,5,1,1300,,,0.532133676,0.461133411,0.071000265
ann,18.19653486,50,5,1,1300,,,0.956925199,0.778335016,0.178590183
ann,1.546246874,50,100,1,1300,,,0.940435137,0.78109329,0.159341847
ann,0.939873739,50,800,1,1300,,,0.889272055,0.768304926,0.120967129
ann,3.109180987,100,100,1,1300,,,0.957614897,0.785105326,0.172509572
ann,2.444917965,100,200,1,1300,,,0.957426798,0.79338015,0.164046647
ann,2.110971224,100,400,1,1300,,,0.95222271,0.798144443,0.154078267
ann,1.835013533,100,800,1,1300,,,0.940560537,0.799899709,0.140660828
ann,3.653039066,200,800,1,1300,,,0.959558593,0.79663993,0.162918663
Number of epochs affect variance,,,,,,,,,,
"GPU only at around 18% usage during training. Most likely because it's processing faster than it can be fed data. Read speed to GPU was bottleneck, and increasing batch_size helped without affecting performances on training and validation set by too much.",,,,,,,,,,
,,,,,,,,,,
ann,1.546246874,50,100,1,1300,,,0.940435137,0.78109329,0.159341847
ann,1.808737385,50,100,2,1300,,,0.950279014,0.772818466,0.177460548
ann,2.574378614,50,100,5,1300,,,0.92181328,0.738214656,0.183598624
No improved performance with more layers. 1 hidden layer is enough.,,,,,,,,,,
,,,,,,,,,,
ann,1.796250892,100,800,1,1000,,,0.93466675,0.793380151,0.141286599
ann,1.835013533,100,800,1,1300,,,0.940560537,0.799899709,0.140660828
ann,1.876340163,100,800,1,1600,,,0.944886827,0.800401214,0.144485613
ann,2.000206093,100,800,1,2200,,,0.949714716,0.800151966,0.14906275
ann,2.506201291,100,800,3,2200,,,0.958053796,0.766549659,0.191504137
More neurons slightly better. No improved performance with more layers with more hidden units.,,,,,,,,,,
,,,,,,,,,,
L2 regularization in hidden layer,,,,,,,,,,
ann,1.885158006,100,800,1,1600,0.01,,0.555395323,0.47216651,0.083228812
ann,1.907134505,100,800,1,1600,0.003,,0.697723995,0.608826492,0.088897503
L2 regularization not working well,,,,,,,,,,
,,,,,,,,,,
Dropout regularization in hidden layer,,,,,,,,,,
ann,1.93950992,100,800,1,1600,,0,0.944573327,0.800902718,0.143670609
ann,1.885923743,100,800,1,1600,,0.2,0.938115242,0.798645948,0.139469294
ann*,1.905453177,100,800,1,1600,,0.4,0.938115242,0.799147452,0.13896779
ann,1.908396184,100,800,1,1600,,0.4,0.944636027,0.800902718,0.143733309
ann,3.746803522,200,800,1,1600,,0.4,0.955922001,0.79663993,0.159282072
Dropout regularization not working well,,,,,,,,,,
*without constraint on the maximum norm of hidden layer weight matrices,,,,,,,,,,
